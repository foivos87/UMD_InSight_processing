{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download InSight data from IPGP portal ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is a script for the UMD group of InSight, for downloading and treating (rotation and instrument response removal) the InSight data that are available on the dedicated portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Contributors: Foivos Karakostas, Doyeon Kim, Ross Maguire and the UMD InSight group*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first input is the **Event name**. The events are cataloged with the number of the Sol (Martian Day), followed by a letter (a, b, c, ...) which indicates if it is the 1st, 2nd, 3rd, etc. event of that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter event: S0133a\n"
     ]
    }
   ],
   "source": [
    "event2find = input('Enter event: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the user should select the type of the output. When the instrument response removal is performed, the output can be given in terms of ground displacement, velocity or acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose output DISP / VEL / ACC: ACC\n"
     ]
    }
   ],
   "source": [
    "comp = input('Choose output DISP / VEL / ACC: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line printed on the screen will indicate that the code is going to perform the calculations for the specific **event** and **output**, both selected by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running code for: Event S0133a and ACC\n"
     ]
    }
   ],
   "source": [
    "print('Running code for: Event ' + event2find + ' and ' + comp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is going to look for the event in the **Marsquake catalog**. This catalog is at the moment a *.txt* file, because this is the only format where the event names, classes and qualities are included. This convention is not included in the *.xml* files, available at the portal, yet. Therefore, the file **A1_Marsquakes_catalog.txt** is necessary to be available at the same directory for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fname = 'A1_Marsquakes_catalog.txt'\n",
    "data = np.loadtxt(fname,dtype=str,usecols=(0,1,2,7)).T\n",
    "\n",
    "eventtime = data[0]\n",
    "event = data[1]\n",
    "eventclass = data[2]\n",
    "quality = data[3]\n",
    "\n",
    "for i in range(0,len(event)):\n",
    "    event1 = str(event[i])\n",
    "    if event1 == event2find:\n",
    "        a=i\n",
    "\n",
    "time_1 = str(eventtime[a])\n",
    "t1l=len(time_1)-3\n",
    "time_2 = time_1\n",
    "class_1 = str(eventclass[a])\n",
    "c1l = len(class_1)-3\n",
    "class_2 = class_1\n",
    "quality_1 = str(quality[a])\n",
    "q1l = len(quality_1)-3\n",
    "quality_2 = quality_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code has found the requested event, its **name**, the associated **time** for the start of the waveform (which is before the P arrivals), its **class** and its **quality**, according to the *MQS convention*. Now it prints it on the screen for a check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: S0133a\n",
      "Datetime of the event: 2019-04-12T18:12:51\n",
      "Class: BROADBAND\n",
      "Quality: B\n"
     ]
    }
   ],
   "source": [
    "print ('Event: ' + event2find)\n",
    "print ('Datetime of the event: ' + time_2)\n",
    "print ('Class: ' + class_2)\n",
    "print ('Quality: ' + quality_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the **time** is converted into the UTC convention, using *ObsPy* in order to prepare the data request and downloading. As the data of the IPGP portal are sometimes missing in the start time (reason unknown), it is wise to define a relatively early start time of the requested waveforms. Therefore, 30 minutes before the time of the event and 90 minutes after this time, is considered a good time window to be sure that the event will be contained in every channel and any application of window function (*hamming, Tukey, etc*) will not have an effect to the part of the data concerning the event waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-12T17:42:51.000000Z\n",
      "2019-04-12T19:42:51.000000Z\n"
     ]
    }
   ],
   "source": [
    "from obspy import UTCDateTime\n",
    "time = UTCDateTime(time_2)\n",
    "    \n",
    "starttime = time - 60*30\n",
    "print(starttime)\n",
    "endtime = time + 60*90\n",
    "print(endtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to proceed to the data download. In order to do this, we should define which kind of data we want. If the user is interested for the VBB InSight data, the channels *BHU, BHV and BHW* should be downloaded. Therefore, we define the appropriate, **network**, **station**, **location** and **channels** for the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = 'XB'\n",
    "sta = 'ELYSE'\n",
    "loc = '02'\n",
    "chan = 'BH*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to download the data. For this we will need the authorization for the IPGP portal, a username and a password. The user is asked to provide them. After the authorization the requested data are saved in an *.mseed* file, named **fdsnws_msds.mseed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give username: foivos@umd.edu\n",
      "Give password: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "username = input('Give username: ')\n",
    "import getpass\n",
    "password = getpass.getpass('Give password: ')\n",
    "\n",
    "url = ('https://ws.seis-insight.eu/fdsnws/dataselect/1/query?network=' + net + '&station=' + sta + '&startTime=' + str(starttime) + '&endTime=' + str(endtime) + '&location=*&channel=' + chan)\n",
    "\n",
    "r = requests.get(url, auth=(username,password))\n",
    "\n",
    "filename=('fdsnws_msds.mseed')\n",
    "\n",
    "if r.status_code == 200:\n",
    "    with open(filename, 'wb') as out:\n",
    "        for bits in r.iter_content():\n",
    "            out.write(bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *.mseed* file is downloaded and the data should be read and added to a *stream*. The stream parameters are printed in the screen in order to check the downloaded channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Trace(s) in Stream:\n",
      "XB.ELYSE.02.BHU | 2019-04-12T17:42:38.015000Z - 2019-04-12T18:00:52.165000Z | 20.0 Hz, 21884 samples\n",
      "XB.ELYSE.02.BHU | 2019-04-12T18:00:00.215000Z - 2019-04-12T18:40:04.165000Z | 20.0 Hz, 48080 samples\n",
      "XB.ELYSE.02.BHU | 2019-04-12T18:39:12.214000Z - 2019-04-12T19:00:52.164000Z | 20.0 Hz, 26000 samples\n",
      "XB.ELYSE.02.BHU | 2019-04-12T19:00:00.214000Z - 2019-04-12T19:30:52.164000Z | 20.0 Hz, 37040 samples\n",
      "XB.ELYSE.02.BHU | 2019-04-12T19:30:00.212000Z - 2019-04-12T19:42:57.562000Z | 20.0 Hz, 15548 samples\n",
      "XB.ELYSE.02.BHV | 2019-04-12T17:42:40.566000Z - 2019-04-12T18:00:52.166000Z | 20.0 Hz, 21833 samples\n",
      "XB.ELYSE.02.BHV | 2019-04-12T18:00:00.215000Z - 2019-04-12T18:40:04.165000Z | 20.0 Hz, 48080 samples\n",
      "XB.ELYSE.02.BHV | 2019-04-12T18:39:12.214000Z - 2019-04-12T19:00:52.164000Z | 20.0 Hz, 26000 samples\n",
      "XB.ELYSE.02.BHV | 2019-04-12T19:00:00.214000Z - 2019-04-12T19:30:52.164000Z | 20.0 Hz, 37040 samples\n",
      "XB.ELYSE.02.BHV | 2019-04-12T19:30:00.212000Z - 2019-04-12T19:42:51.362000Z | 20.0 Hz, 15424 samples\n",
      "XB.ELYSE.02.BHW | 2019-04-12T17:42:34.616000Z - 2019-04-12T18:00:52.166000Z | 20.0 Hz, 21952 samples\n",
      "XB.ELYSE.02.BHW | 2019-04-12T18:00:00.215000Z - 2019-04-12T18:40:04.165000Z | 20.0 Hz, 48080 samples\n",
      "XB.ELYSE.02.BHW | 2019-04-12T18:39:12.214000Z - 2019-04-12T19:00:52.164000Z | 20.0 Hz, 26000 samples\n",
      "XB.ELYSE.02.BHW | 2019-04-12T19:00:00.214000Z - 2019-04-12T19:30:52.164000Z | 20.0 Hz, 37040 samples\n",
      "XB.ELYSE.02.BHW | 2019-04-12T19:30:00.212000Z - 2019-04-12T19:42:58.662000Z | 20.0 Hz, 15570 samples\n"
     ]
    }
   ],
   "source": [
    "import obspy\n",
    "from obspy import read\n",
    "\n",
    "st = read('fdsnws_msds.mseed')\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes more than one traces are available for one component, therefore a merge is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Streams after merging\n",
      "----------------------\n",
      "3 Trace(s) in Stream:\n",
      "XB.ELYSE.02.BHU | 2019-04-12T17:42:38.015000Z - 2019-04-12T19:42:57.565000Z | 20.0 Hz, 144392 samples\n",
      "XB.ELYSE.02.BHV | 2019-04-12T17:42:40.566000Z - 2019-04-12T19:42:51.366000Z | 20.0 Hz, 144217 samples\n",
      "XB.ELYSE.02.BHW | 2019-04-12T17:42:34.616000Z - 2019-04-12T19:42:58.666000Z | 20.0 Hz, 144482 samples\n"
     ]
    }
   ],
   "source": [
    "for tr in st.select(component='U'):\n",
    "    st.merge(tr)  \n",
    "print ('----------------------')\n",
    "print('Streams after merging')\n",
    "print ('----------------------')\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On can notice that the channel streams do not have the same starttime and endtime. However, in order to perform the inversion, they should be all of the same length, with equal start and end times. Therefore, a trimming is applied, starting at the latest starttime and ending at the earlier endtime. In the end, the new channel properties are printed on the screen, in order to check that the channel streams have equal starrtime, endtime and number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streams after trimming\n",
      "----------------------\n",
      "3 Trace(s) in Stream:\n",
      "XB.ELYSE.02.BHU | 2019-04-12T17:42:40.565000Z - 2019-04-12T19:42:51.365000Z | 20.0 Hz, 144217 samples\n",
      "XB.ELYSE.02.BHV | 2019-04-12T17:42:40.566000Z - 2019-04-12T19:42:51.366000Z | 20.0 Hz, 144217 samples\n",
      "XB.ELYSE.02.BHW | 2019-04-12T17:42:40.566000Z - 2019-04-12T19:42:51.366000Z | 20.0 Hz, 144217 samples\n"
     ]
    }
   ],
   "source": [
    "timeU1=st[0].stats.starttime;\n",
    "timeV1=st[1].stats.starttime;\n",
    "timeW1=st[2].stats.starttime;\n",
    "\n",
    "stime=max(timeU1, timeV1, timeW1)\n",
    "\n",
    "timeUe=st[0].stats.endtime;\n",
    "timeVe=st[1].stats.endtime;\n",
    "timeWe=st[2].stats.endtime;\n",
    "\n",
    "etime=min(timeUe, timeVe, timeWe)\n",
    "\n",
    "st.trim(stime, etime);\n",
    "\n",
    "print ('Streams after trimming')\n",
    "print ('----------------------')\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the aformentioned statement is true for the set of channels, the instrument response removal is ready to be performed. In the beginning of the code, the user has already chosen which seismogram wants to obtain, **ground displacement** *(DISP)*, **velocity** *(VEL)* or **acceleration** *(ACC)*. The information for the instrument response is found in the **dataless.XB.ELYSE.seed** file. *(Note: the instrument response is performed by applying a prefiltering in the data, a taper window which is the appropriate for 20 Hz data)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import read_inventory\n",
    "\n",
    "inv = obspy.read_inventory('dataless.XB.ELYSE.seed')\n",
    "\n",
    "st_rem1=st.copy()\n",
    "pre_filt = [0.005, 0.01, 8, 10] #for 20 Hz data\n",
    "st_rem1.remove_response(output = comp, taper_fraction=0.05, pre_filt = pre_filt, inventory = inv);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the instrument response removal is performed, the data should be rotated from the **BHU**, **BHV** and **BHW** channels to **BHE**, **BHN** and **BHZ**. In order to do this, the information available in the *dataless* file, which is already loaded as the *inventory* is necessary. More precisely, the *dip* and *azimuth* of the channels is going to be used in order to perform the rotation of the stream data. The code will print in the screen the channel properties that are taken from the *inventory* and then it will perform the rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHNDATA--------------------------------\n",
      "Channel 'BHU', Location '02' \n",
      "\tTime range: 2019-02-12T02:43:01.476000Z - 2019-02-28T07:57:28.595000Z\n",
      "\tLatitude: 4.50, Longitude: 135.62, Elevation: 0.0 m, Local Depth: -0.1 m\n",
      "\tAzimuth: 135.10 degrees from north, clockwise\n",
      "\tDip: -29.40 degrees down from horizontal\n",
      "\tSampling Rate: 20.00 Hz\n",
      "\tSensor (Description): VBB Velocity SCI mode (None)\n",
      "\tResponse information available\n",
      "CHNDATA--------------------------------\n",
      "Channel 'BHV', Location '02' \n",
      "\tTime range: 2019-02-12T02:43:01.476000Z - 2019-02-28T07:57:28.595000Z\n",
      "\tLatitude: 4.50, Longitude: 135.62, Elevation: 0.0 m, Local Depth: -0.1 m\n",
      "\tAzimuth: 15.00 degrees from north, clockwise\n",
      "\tDip: -29.20 degrees down from horizontal\n",
      "\tSampling Rate: 20.00 Hz\n",
      "\tSensor (Description): VBB Velocity SCI mode (None)\n",
      "\tResponse information available\n",
      "CHNDATA--------------------------------\n",
      "Channel 'BHW', Location '02' \n",
      "\tTime range: 2019-02-12T02:43:01.476000Z - 2019-02-28T07:57:28.595000Z\n",
      "\tLatitude: 4.50, Longitude: 135.62, Elevation: 0.0 m, Local Depth: -0.1 m\n",
      "\tAzimuth: 255.00 degrees from north, clockwise\n",
      "\tDip: -29.70 degrees down from horizontal\n",
      "\tSampling Rate: 20.00 Hz\n",
      "\tSensor (Description): VBB Velocity SCI mode (None)\n",
      "\tResponse information available\n"
     ]
    }
   ],
   "source": [
    "sta = inv[0][0]\n",
    "azs = []\n",
    "dips = []\n",
    "trs = []\n",
    "\n",
    "channels = ['BHU','BHV','BHW']\n",
    "\n",
    "for chn in channels:\n",
    "    chndata = sta.select(channel=chn)[0]\n",
    "    print ('CHNDATA--------------------------------')\n",
    "    print (chndata)\n",
    "    azs.append(chndata.azimuth)\n",
    "    dips.append(chndata.dip)\n",
    "    \n",
    "from obspy.signal.rotate import rotate2zne\n",
    "    \n",
    "(z, n, e) = rotate2zne(st_rem1[0], azs[0], dips[0], st_rem1[1], azs[1], dips[1], st_rem1[2], azs[2], dips[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tukey window with 5% of data in the sin function, is applied, in order to remove the artifacts of the high amplitudes at the edges of the waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "lenz = len(z)\n",
    "alp = 5e-2\n",
    "window = signal.tukey(len(z), alpha = alp)\n",
    "z = z * window\n",
    "n = n * window\n",
    "e = e * window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data for the vertical and horizontal components are available and ready to be added into a new stream, which is the final product of this data processing. The code will print in the end the stats of the new channels with the new names introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "         network: XB\n",
      "         station: ELYSE\n",
      "        location: 02\n",
      "         channel: BHZ\n",
      "       starttime: 2019-04-12T17:42:40.565000Z\n",
      "         endtime: 2019-04-12T19:42:51.365000Z\n",
      "   sampling_rate: 20.0\n",
      "           delta: 0.05\n",
      "            npts: 144217\n",
      "           calib: 1.0\n",
      "         _format: MSEED\n",
      "           mseed: AttribDict({'dataquality': 'R', 'number_of_records': 52, 'encoding': 'STEIM2', 'byteorder': '>', 'record_length': 512, 'filesize': 543232})\n",
      "      processing: ['ObsPy 1.2.2: trim(endtime=UTCDateTime(2019, 4, 12, 19, 42, 51, 365000)::fill_value=None::nearest_sample=True::pad=False::starttime=UTCDateTime(2019, 4, 12, 17, 42, 40, 565000))', \"ObsPy 1.2.2: remove_response(fig=None::inventory=<obspy.core.inventory.inventory.Inventory object at 0x1266ac438>::output='ACC'::plot=False::pre_filt=[0.005, 0.01, 8, 10]::taper=True::taper_fraction=0.05::water_level=60::zero_mean=True)\"]\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "         network: XB\n",
      "         station: ELYSE\n",
      "        location: 02\n",
      "         channel: BHN\n",
      "       starttime: 2019-04-12T17:42:40.566000Z\n",
      "         endtime: 2019-04-12T19:42:51.366000Z\n",
      "   sampling_rate: 20.0\n",
      "           delta: 0.05\n",
      "            npts: 144217\n",
      "           calib: 1.0\n",
      "         _format: MSEED\n",
      "           mseed: AttribDict({'dataquality': 'R', 'number_of_records': 52, 'encoding': 'STEIM2', 'byteorder': '>', 'record_length': 512, 'filesize': 543232})\n",
      "      processing: ['ObsPy 1.2.2: trim(endtime=UTCDateTime(2019, 4, 12, 19, 42, 51, 365000)::fill_value=None::nearest_sample=True::pad=False::starttime=UTCDateTime(2019, 4, 12, 17, 42, 40, 565000))', \"ObsPy 1.2.2: remove_response(fig=None::inventory=<obspy.core.inventory.inventory.Inventory object at 0x1266ac438>::output='ACC'::plot=False::pre_filt=[0.005, 0.01, 8, 10]::taper=True::taper_fraction=0.05::water_level=60::zero_mean=True)\"]\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "         network: XB\n",
      "         station: ELYSE\n",
      "        location: 02\n",
      "         channel: BHE\n",
      "       starttime: 2019-04-12T17:42:40.566000Z\n",
      "         endtime: 2019-04-12T19:42:51.366000Z\n",
      "   sampling_rate: 20.0\n",
      "           delta: 0.05\n",
      "            npts: 144217\n",
      "           calib: 1.0\n",
      "         _format: MSEED\n",
      "           mseed: AttribDict({'dataquality': 'R', 'number_of_records': 52, 'encoding': 'STEIM2', 'byteorder': '>', 'record_length': 512, 'filesize': 543232})\n",
      "      processing: ['ObsPy 1.2.2: trim(endtime=UTCDateTime(2019, 4, 12, 19, 42, 51, 365000)::fill_value=None::nearest_sample=True::pad=False::starttime=UTCDateTime(2019, 4, 12, 17, 42, 40, 565000))', \"ObsPy 1.2.2: remove_response(fig=None::inventory=<obspy.core.inventory.inventory.Inventory object at 0x1266ac438>::output='ACC'::plot=False::pre_filt=[0.005, 0.01, 8, 10]::taper=True::taper_fraction=0.05::water_level=60::zero_mean=True)\"]\n"
     ]
    }
   ],
   "source": [
    "st_new1=st_rem1.copy()\n",
    "st_new1[0].data = z;\n",
    "st_new1[0].stats.channel = 'BHZ'\n",
    "st_new1[1].data = n;\n",
    "st_new1[1].stats.channel = 'BHN'\n",
    "st_new1[2].data = e;\n",
    "st_new1[2].stats.channel = 'BHE'\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "print(st_new1[0].stats)\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "print(st_new1[1].stats)\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -')\n",
    "print(st_new1[2].stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained stream is going to make new *.mseed* files, following the naming convention **eventname_channel_COMP.mseed** and save it in a directory which follows the naming convention **Treated/Class/Quality/Eventname/**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/obspy/io/mseed/core.py:790: UserWarning: The encoding specified in trace.stats.mseed.encoding does not match the dtype of the data.\n",
      "A suitable encoding will be chosen.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = ('Treated/')\n",
    "check = os.path.isdir(path)\n",
    "if check == False:\n",
    "    os.mkdir(path)\n",
    "path = ('Treated/' + class_2 + '/')\n",
    "check = os.path.isdir(path)\n",
    "if check == False:\n",
    "    os.mkdir(path)\n",
    "path = ('Treated/' + class_2 + '/' + quality_2 + '/')\n",
    "check = os.path.isdir(path)\n",
    "if check == False:\n",
    "    os.mkdir(path)\n",
    "path = ('Treated/' + class_2 + '/' + quality_2 + '/' + event2find + '/')\n",
    "check = os.path.isdir(path)\n",
    "if check == False:\n",
    "    os.mkdir(path)\n",
    "filename1 = (path + '/' + event2find + '_' + comp + '.mseed')\n",
    "st_new1.write(filename1, format = 'MSEED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end of the code there will be a warning, but this is not of great importance, as the *.mseed* files are found to be created and saved consistently approprietly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may ignore this warning\n"
     ]
    }
   ],
   "source": [
    "print('You may ignore this warning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
